{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning\n",
        "Fine tuning is a form of transfer learning. Fine-tuning updates a model that has already learned general information with new data, to specialize it. Gradually fine-tune different layers of the model, starting with the topmost layers and progressively including lower layers is one of the techniques used traditionally in while fine-tuning the model. However there are challanges to adapt traditional methods. The challanges are:\n",
        "1. Gathering labelled data\n",
        "1. Computational Cost\n",
        "1. Storage \n",
        "1. Out-of-distribution data (leads to Catastrophic Forgetting)\n",
        "\n",
        "**Catastrophic Forgetting**: This happens when a model learns something new but forgets what it learned before. Imagine if you crammed for a history test and did great, but then forgot most of what you learned when you started studying for a math test.\n",
        "\n",
        "One of the solutions to above challenges is parameter efficient fine-tuning(PEFT). \n",
        "\n",
        "__What is PEFT?__\n",
        "\n",
        "A method of updating a predefined subset of a model's parameters to tailor it to specific tasks, without the need to modify the entire model, thus saving computational resources is called parameter-efficient fine-tuning. There are several PEFT techinques. Some of them are discussed below. Note: In this notebook we will use LoRA. \n",
        "\n",
        "## PEFT techinques: \n",
        "### Low-Rank Adaptation (LoRA)\n",
        "\n",
        "Low-Rank Adaptation (LoRA) is a technique used in deep learning to efficiently fine-tune pre-trained models by decomposing their weight matrices into low-rank representations. This method significantly reduces the number of trainable parameters, making the fine-tuning process more efficient while retaining the model's performance.\n",
        "\n",
        "<img src=\"./images/image.png\" width=\"550\">\n",
        "\n",
        "Suppose $\\Delta W$ is the weight update for an $A \\times B$ weight matrix. Then, we can decompose the weight update matrix into two smaller matrices: $\\Delta W = W_A W_B$ , where $W_A$ is an an $A \\times r$-dimensional matrix, and $W_B$ is an an $r \\times B$-dimensional matrix. Here, we keep the original weight $W$ frozen and only train the new matrices $W_A$ and $W_B$. This, in a nutshell, is the LoRA method, which is illustrated in the figure below.\n",
        "\n",
        "### Choosing the Rank in LoRA\n",
        "\n",
        "The rank $r$ in LoRA is a hyperparameter that specifies the complexity of the low-rank matrices used for adaptation. Here's a summary of how choosing $r$ affects the model:\n",
        "\n",
        "- **Smaller $r$**:\n",
        "  - **Pros**: Fewer parameters to learn, faster training, reduced computational requirements.\n",
        "  - **Cons**: Lower capacity to capture task-specific information, potentially leading to lower adaptation quality and poorer performance on the new task.\n",
        "\n",
        "- **Larger $r$**:\n",
        "  - **Pros**: Higher capacity to capture task-specific information, potentially better performance on the new task.\n",
        "  - **Cons**: Increased model complexity, more parameters to learn, longer training times, and higher computational requirements.\n",
        "\n",
        "Choosing the right $r$ involves balancing model complexity and adaptation capacity to avoid underfitting or overfitting. Experimentation with different $r$ values is crucial to finding the optimal balance for the desired task performance.\n",
        "\n",
        "---\n",
        "Prior to proposal of LoRA two main parameter efficient finetuning approaches were there: \n",
        "1. Adapters \n",
        "2. Prefix tuning \n",
        "\n",
        "### Adapters: \n",
        "The adapter blocks are extra trainable modules inserted into the existing transformer block. Adapter blocks are inserted after both attention and feedforward layers that have a small number of parameters and can be finetuned while keeping the weights of the pretrained model fixed.\n",
        "\n",
        "  Each adapter block is a bottleneck-style feedforward module that \n",
        "  - decreases the dimensionality of the input via a (trainable) linear layer, \n",
        "  - applies a non-linearity, and \n",
        "  - restores the original dimensionality of the input via a final (trainable) linear layer.\n",
        "\n",
        "Only the parameters of these adapters are trained, not of the entire model.\n",
        "\n",
        "<img src=\"./images/adapter_approach.png\" width=\"400\">\n",
        "\n",
        "### Prefix Tuning\n",
        "\n",
        "  Another parameter-efficient finetuning alternative is prefix tuning, which keeps the language model’s parameters frozen and only finetunes a few (trainable) token vectors that are added to the beginning of the model’s input sequence. These token vectors can be interchanged for different tasks.\n",
        "\n",
        "---\n",
        "### Comparison between LoRA and Adapters\n",
        "LoRA (Low-Rank Adaptation) and Adapters are both techniques designed to make the fine-tuning of large pre-trained models more efficient, but they are distinct in their implementation and focus.\n",
        "\n",
        "- **Similarity**: Both LoRA and Adapters are designed to make fine-tuning more parameter-efficient and computationally efficient by minimizing the number of parameters that need to be updated.\n",
        "- **Difference**: The key difference lies in their implementation:\n",
        "  - LoRA modifies the internal structure of existing weight matrices by introducing low-rank updates.\n",
        "  - Adapters add new, small trainable modules to the existing model architecture.\n",
        "\n",
        "In summary, while LoRA and Adapters share similar goals in improving fine-tuning efficiency, they achieve this through different mechanisms. LoRA can be thought of as an internal adjustment to the weight matrices, whereas Adapters add external components to the model.\n",
        "\n",
        "Let's use the LoRA to do the PEFT of the foundation model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Igz4eZRV37TE"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset_builder, load_dataset\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "gvK2mrz_37TI",
        "outputId": "7e78a8a2-d97c-4070-9233-0b3d2e5100d6"
      },
      "outputs": [],
      "source": [
        "# Get the dataset movie review dataset from the rotten tomatoes\n",
        "\n",
        "# First let's get the general infomration about the dataset\n",
        "ds_builder = load_dataset_builder(\"cornell-movie-review-data/rotten_tomatoes\")\n",
        "\n",
        "# Inspect the dataset information\n",
        "ds_builder.info.description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzbEFb3_37TI",
        "outputId": "61fbe3a1-b4a9-4ac8-be00-be814a900821"
      },
      "outputs": [],
      "source": [
        "# Insepect the dataset splits\n",
        "ds_builder.info.splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME9EMXzi37TJ",
        "outputId": "73e1e66e-1c4c-4c8e-9bbc-da5de0987c47"
      },
      "outputs": [],
      "source": [
        "# Inspect the features of the dataset\n",
        "ds_builder.info.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "774LjdSU37TJ",
        "outputId": "ab05c05b-c5d9-4cfd-d716-d84bc5d98a44"
      },
      "outputs": [],
      "source": [
        "# Get the list of dataset's split name\n",
        "from datasets import get_dataset_split_names\n",
        "split_names = get_dataset_split_names(\"cornell-movie-review-data/rotten_tomatoes\")\n",
        "print(\"Available splits:\", split_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvMcgiKD37TJ"
      },
      "outputs": [],
      "source": [
        "# Le'ts load the actual dataset\n",
        "# We need all train, validation and test splits so\n",
        "dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBEUQDUV37TJ",
        "outputId": "880515f9-6b8e-424d-e2a6-1cebdf8e88e9"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4shgrpJX37TJ",
        "outputId": "8de028b1-de18-4014-cc5d-bf63a3a3acc3"
      },
      "outputs": [],
      "source": [
        "# Get the first row in the dataset\n",
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZTVAuGJ37TK",
        "outputId": "368cfb4d-f029-47d0-c103-36a1be17373d"
      },
      "outputs": [],
      "source": [
        "# Get the last row in the dataset\n",
        "dataset['train'][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXkKw-aT37TK"
      },
      "source": [
        "## Preprocess the dataset\n",
        "Pre-process the dataset by converting all the text inot tokens for our models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UPHlGlz37TK"
      },
      "outputs": [],
      "source": [
        "# Get the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fGzhG3Qm37TK",
        "outputId": "9891acdb-ea4b-4f34-d317-cc33372792ec"
      },
      "outputs": [],
      "source": [
        "# Let's see the eos token\n",
        "tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFqQcDi937TK"
      },
      "outputs": [],
      "source": [
        "# Let's see the pad token\n",
        "# GPT2 does not have a pad token by default, so we will set it to the eos token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov_GeEVd37TK",
        "outputId": "b94ddfe9-0c65-4c8d-deb7-5ee2257d59fe"
      },
      "outputs": [],
      "source": [
        "# Let's see the pad token\n",
        "print(\"Pad token:\", tokenizer.pad_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "bc5ba5e708c4441ea97ceb78ca22b313",
            "bea4f2c5222a4dbc90dff37772379e79",
            "f4a9f1d1c03f42b598d09c1a13f15565",
            "3aff379c7bc241aa93df512376d6753c",
            "1d87b2733d7543ed9ed6d09d70a099c1",
            "f012ae182b4947c68a52fbbb6ea5473d",
            "ad52d1b1b626472aace70f78c586b204",
            "351946fadb9547f89971f354eaca58cc",
            "eea41d1d20a2433dbc9fdc5919fb6329",
            "e062203706b14beaad78a22ca007ff33",
            "e3e5196310224bb6a2739c1fb0ca4029"
          ]
        },
        "id": "F5PwJ-R237TK",
        "outputId": "69d8eac1-c51a-4662-929f-899cb5acc27b"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = {}\n",
        "\n",
        "for split in split_names:\n",
        "    # Tokenize the text data\n",
        "    tokenized_dataset[split] = dataset[split].map(\n",
        "        lambda examples: tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128),\n",
        "        batched=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPXBN0R637TL",
        "outputId": "cb9413e4-1e6b-44be-94d8-40053c06b3cd"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkZEgA04RtjR"
      },
      "source": [
        "Warning from the hugging face:\n",
        "Model can accept multiple label arguments but none of them should be named \"label\". This is applicable if we use `label_names` as argument in the `TrainingArguments`. So, we will change the \"label\" from our dataset to \"labels\".  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE6KwLH9Ny7b"
      },
      "outputs": [],
      "source": [
        "# Rename 'label' to 'labels' across all splits using a dictionary comprehension\n",
        "tokenized_dataset = {split: ds.rename_column(\"label\", \"labels\") for split, ds in tokenized_dataset.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vADvof0wOt9f",
        "outputId": "ff6f3bf6-7181-4403-db1c-2252983539ea"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX4rP6ba37TL",
        "outputId": "9696f063-e9c0-455c-eebe-9e7cf15f6cdb"
      },
      "outputs": [],
      "source": [
        "# Let's get the label2id and id2label mappings\n",
        "label2id = {label: i for i, label in enumerate(dataset['train'].features['label'].names)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "print(\"Label to ID mapping:\", label2id)\n",
        "print(\"ID to Label mapping:\", id2label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW6i93DhEzrl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# After training, save id2label with your model\n",
        "output_path = \"/content/drive/MyDrive/NepGPT/PEFT/lora_adapters\"  # or full_merged_model\n",
        "with open(f\"{output_path}/id2label.json\", \"w\") as f:\n",
        "    json.dump(id2label, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geX3nO0-37TL"
      },
      "source": [
        "## Get the model for sequence classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzv8qAdV37TL",
        "outputId": "62528c4f-2966-4462-f8ab-cbcf2a3fe719"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\",\n",
        "                                            num_labels=2,  # Binary Classification\n",
        "                                             device_map=\"auto\", # Automatically map to available device\n",
        "                                             id2label=id2label,\n",
        "                                             label2id=label2id)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id  # Set the pad token id in the model config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYCPn8xwTDO5"
      },
      "source": [
        "You might get this warning:\n",
        "`Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.`\n",
        "What is this warning? GPT2 model is used for generating the text not for the classification. But we are using the `gpt2` for classification. So we have score layer in the below architecture which is randomly initialized that acts as the classification head.\n",
        "```python\n",
        "GPT2ForSequenceClassification(\n",
        "  (transformer): GPT2Model(\n",
        "    (wte): Embedding(50257, 768)\n",
        "    (wpe): Embedding(1024, 768)\n",
        "    (drop): Dropout(p=0.1, inplace=False)\n",
        "    (h): ModuleList(\n",
        "      (0-11): 12 x GPT2Block(\n",
        "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        (attn): GPT2Attention(\n",
        "          (c_attn): Conv1D(nf=2304, nx=768)\n",
        "          (c_proj): Conv1D(nf=768, nx=768)\n",
        "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
        "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
        "        )\n",
        "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        (mlp): GPT2MLP(\n",
        "          (c_fc): Conv1D(nf=3072, nx=768)\n",
        "          (c_proj): Conv1D(nf=768, nx=3072)\n",
        "          (act): NewGELUActivation()\n",
        "          (dropout): Dropout(p=0.1, inplace=False)\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "  )\n",
        "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
        ")\n",
        "```\n",
        "When we fin-tune this model using LoRA from the peft library we only choose to fine-tune the layers as shown in the config below:\n",
        "```python\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                    # Rank of the adaptation\n",
        "    lora_alpha=32,          # Scaling factor\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # layers to fine-tune in GPT-2\n",
        "    lora_dropout=0.1,       # Dropout for regularization\n",
        "    fan_in_fan_out=True,    # Match Conv1D expectation, eliminates the warning\n",
        "    task_type=\"SEQ_CLS\"   # For sequence Classification\n",
        ")\n",
        "```\n",
        "Since `score.weight` isn’t present in the gpt2 checkpoint, Hugging Face initializes it randomly when you load `AutoModelForSequenceClassification`. This is what triggers the warning.\n",
        "\n",
        "How to avoid this warning?\n",
        "We can completely ignore this warning as it is seen just while loading the base model before it is fine-tuned and warning the user you might have to TRAIN this model.\n",
        "\n",
        "Also we need to keep in mind that when we initialize the model with `peft_model = PeftModel.from_pretrained(base_model, lora_config)`, the score layer remains a regular Linear layer (not adapted by LoRA).\n",
        "\n",
        "During training, the optimizer (controlled by Trainer) updates `score.weight` fully because unless not frozen or adapted by LoRA. We can check wether the it is optimized or not by\n",
        "```python\n",
        "# After initializing peft_model\n",
        "print(\"Is score.weight trainable?\", peft_model.score.weight.requires_grad)  # Should print True\n",
        "```\n",
        "Therefore, later when you load the adapter after fine-tuning with the base model the completely fine-tuned `score.weight` get's loaded. No need to worry about the warning at all.  \n",
        "\n",
        "Also to be extra cautious we can check after training and after saving the model:\n",
        "```python\n",
        "# After training\n",
        "trainer.train()\n",
        "post_training_score = peft_model.score.weight[0][:5].tolist()\n",
        "print(\"Post-training score.weight:\", post_training_score)\n",
        "peft_model.save_pretrained(output_path)\n",
        "tokenizer.save_pretrained(output_path)\n",
        "\n",
        "# Reload\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\")\n",
        "peft_model = PeftModel.from_pretrained(base_model, output_path)\n",
        "loaded_score = peft_model.score.weight[0][:5].tolist()\n",
        "print(\"Loaded score.weight:\", loaded_score)\n",
        "print(\"Match?\", post_training_score == loaded_score)\n",
        "```\n",
        "If `Match? True`, warning is just noise, and model is working as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e08HLLUD37TL",
        "outputId": "0590fdc0-e9cf-4e4c-e93d-015cbc9e19b8"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzNIxqGn37TM"
      },
      "source": [
        "- `c_attn`: The attention layer’s combined query/key/value projection.\n",
        "- `c_proj`: The attention output projection.\n",
        "- `c_fc`: The feed-forward network’s first projection (expanding the hidden size).\n",
        "- `c_proj (in MLP)`: The feed-forward network’s second projection (reducing back to the hidden size).\n",
        "- `score`: Output head for the binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmP0F54N37TM"
      },
      "outputs": [],
      "source": [
        "def print_model_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total Number of Parameters: {total_params:,}\")\n",
        "    actual_total_params = (\n",
        "        total_params - sum(\n",
        "            p.numel() for p in model.score.parameters()\n",
        "        )\n",
        "    )\n",
        "\n",
        "    total_size_bytes = total_params * 4\n",
        "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "\n",
        "    print(f\"Actual Number of trainable parameters \"\n",
        "          f\"considering weight tying: {actual_total_params:,}\\n\"\n",
        "          f\"Total size of the model is: {total_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kJ65fa737TM",
        "outputId": "c4fea9eb-4de8-4172-bb02-c617b7bcb4d2"
      },
      "outputs": [],
      "source": [
        "print_model_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laI9A3SQ_e3K"
      },
      "outputs": [],
      "source": [
        "# Let's define the compute metrics function\n",
        "# Compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = (predictions == labels).mean()\n",
        "    return {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJx3xqDX37TN"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    # Checkpoint Saving Settings\n",
        "    output_dir=\"./gpt2-lora-rotten-tomatoes-classification/\",\n",
        "    overwrite_output_dir=True,  # Overwrites the output directory if it already exists, avoiding conflicts.\n",
        "\n",
        "    # Dataset labels\n",
        "    label_names = ['labels'],\n",
        "\n",
        "    # Training Epoch and Batch Settings\n",
        "    num_train_epochs=2,  # Runs training for 3 full passes over the dataset.\n",
        "    per_device_train_batch_size=8,  # Uses a batch size of 8 per device (e.g., GPU/CPU) during training.\n",
        "    per_device_eval_batch_size=8,  # Sets evaluation batch size to 8, matching training for consistency.\n",
        "\n",
        "    # Evaluation Configuration\n",
        "    eval_strategy=\"steps\",  # Evaluates the model at regular step intervals rather than epoch ends.\n",
        "    eval_steps=100,  # Performs evaluation every 100 training steps.\n",
        "    eval_delay=100,  # Delays the first evaluation until after 100 steps, allowing initial training to stabilize.\n",
        "\n",
        "\n",
        "    # Model Checkpoint Saving Options\n",
        "    save_strategy=\"steps\",  # Saves checkpoints based on steps, aligning with evaluation strategy.\n",
        "    save_steps=100,  # Saves a checkpoint every 100 steps, syncing with evaluation for easy rollback.\n",
        "    save_safetensors=True,  # Uses safetensors format for safer, faster, and more efficient model storage.\n",
        "    save_total_limit=2,  # Keeps only the 2 most recent checkpoints to manage disk space.\n",
        "\n",
        "    # Optimizer Hyperparameters\n",
        "    learning_rate=2e-4,  # Sets a moderate learning rate (0.0002) suitable for fine-tuning tasks like LoRA.\n",
        "    weight_decay=0.01,  # Applies a small weight decay to prevent overfitting by regularizing weights.\n",
        "\n",
        "    # Logging Configuration\n",
        "    logging_dir=\"./logs\",  # Stores training logs in a dedicated directory for analysis.\n",
        "    logging_steps=10,  # Logs metrics every 10 steps, providing frequent updates on training progress.\n",
        "    logging_strategy=\"steps\",  # Logs based on step intervals, consistent with evaluation and saving.\n",
        "\n",
        "    # Model Selection and Reporting\n",
        "    load_best_model_at_end=True,  # Loads the best-performing model (based on metric) at the end of training.\n",
        "    metric_for_best_model=\"accuracy\",  # Uses accuracy as the criterion to determine the \"best\" model.\n",
        "    report_to=\"none\",  # Disables external reporting (e.g., TensorBoard, WandB) for simplicity or local use.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpRC4-ILqwyB"
      },
      "source": [
        "The `Trainer` expects the metric name specified in `metric_for_best_model` (i.e., \"accuracy\") to exactly match a key in the dictionary returned by `compute_metrics` function.\n",
        "If \"accuracy\" isn’t found in the returned dictionary (e.g., you return `{\"acc\": value}`instead), the Trainer won’t know what value to use to determine the \"best\" model. This could lead to an error.\n",
        "The `load_best_model_at_end` feature will not work as expected, because it can’t compare models based on a missing metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4M1lZB537TN"
      },
      "outputs": [],
      "source": [
        "# Let's create the trainer\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6kIsdK5_wEJ"
      },
      "source": [
        "# How accurate the model is before fine-tuning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "vOovGpvp_vpp",
        "outputId": "f70cedaa-f44a-4935-8d62-7ad082cfe12a"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(tokenized_dataset['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BexADERF37TM"
      },
      "source": [
        "## PEFT Model for Sequence Classification using LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_a5G_-k37TM",
        "outputId": "59c5c232-9891-4696-aa3a-67032f41f7e5"
      },
      "outputs": [],
      "source": [
        "# Let's check the availabe task types from the LoraConfig\n",
        "from peft import TaskType\n",
        "print(\"Available Task Types in LoraConfig:\")\n",
        "for task in TaskType:\n",
        "    print(f\"- {task.name}: {task.value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jVMeetF37TM"
      },
      "outputs": [],
      "source": [
        "# Load configuration for lora\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                    # Rank of the adaptation\n",
        "    lora_alpha=32,          # Scaling factor\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # layers to fine-tune in GPT-2\n",
        "    lora_dropout=0.1,       # Dropout for regularization\n",
        "    fan_in_fan_out=True,    # Match Conv1D expectation, eliminates the warning\n",
        "    task_type=\"SEQ_CLS\"   # For sequence Classification\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l31v2zM337TM"
      },
      "outputs": [],
      "source": [
        "# Convert the transfomer model to a PEFT model\n",
        "peft_model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vz-STyS37TM",
        "outputId": "9b5237aa-0f33-4c89-80da-b44a71b3fd7f"
      },
      "outputs": [],
      "source": [
        "peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0oN3Fw3FN_V",
        "outputId": "eb33dd4f-d42c-4dad-c263-d58988adbc93"
      },
      "outputs": [],
      "source": [
        "# After initializing peft_model\n",
        "print(\"Is score.weight trainable?\", peft_model.score.weight.requires_grad)  # Should print True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eitMwB7peGnl"
      },
      "source": [
        "### Before PEFT: The Standard GPT-2 Model\n",
        "The \"Before PEFT\" version is a straightforward GPT-2 model designed for sequence classification. It starts with a transformer backbone called `GPT2Model`. Inside this, you’ve got:\n",
        "- **Embeddings**: One layer for word tokens (mapping 50,257 tokens to 768 dimensions) and another for positions (mapping 1,024 positions to 768 dimensions).\n",
        "- **Dropout**: A dropout layer with a 10% probability to prevent overfitting.\n",
        "- **Transformer Blocks**: 12 identical blocks stacked together. Each block has:\n",
        "  - Two **LayerNorm** layers to stabilize training (normalizing 768-dimensional vectors).\n",
        "  - An **attention mechanism** with two key parts: `c_attn` (a 1D convolution transforming 768 dimensions to 2,304) and `c_proj` (another convolution bringing it back to 768), both with dropout layers (10%) for regularization.\n",
        "  - A **multi-layer perceptron (MLP)** with `c_fc` (convolution from 768 to 3,072 dimensions), a GELU activation function, and `c_proj` (convolution back from 3,072 to 768), plus dropout.\n",
        "- **Final LayerNorm**: Another normalization layer at the end of the transformer.\n",
        "- **Score Layer**: A simple linear layer that takes the 768-dimensional output and maps it to 2 classes (e.g., positive/negative), without bias.\n",
        "\n",
        "In this setup, every single parameter—about 124 million for GPT-2—is trainable. When you fine-tune this model, you’re adjusting all those weights, which takes a lot of memory and computation.\n",
        "\n",
        "---\n",
        "\n",
        "### After PEFT: The LoRA-Enhanced Model\n",
        "The \"After PEFT\" version builds on the same GPT-2 foundation but introduces **Parameter-Efficient Fine-Tuning (PEFT)** using **LoRA (Low-Rank Adaptation)**. Here’s how it changes:\n",
        "\n",
        "- **Top-Level Wrapper**: Instead of just `AutoModelForSequenceClassification`, it’s now a `PeftModelForSequenceClassification` with a `LoraModel` inside. This is the PEFT framework managing the efficiency tweaks.\n",
        "- **Core Structure**: The base `GPT2Model` stays almost identical—same embeddings (50,257 tokens and 1,024 positions to 768 dimensions), same dropout (10%), same 12 transformer blocks, and same LayerNorm layers. The overall layout doesn’t change much.\n",
        "- **Attention Changes**: In the attention mechanism:\n",
        "  - `c_attn` (originally a convolution from 768 to 2,304) is now wrapped in a `lora.Linear` layer. This keeps the original convolution (called the \"base layer\") but adds two tiny trainable layers: `lora_A` (768 to 8 dimensions) and `lora_B` (8 back to 2,304). These are low-rank matrices, meaning they’re much smaller than the original.\n",
        "  - `c_proj` (originally 768 to 768) gets the same treatment: a `lora.Linear` wrapper with `lora_A` (768 to 8) and `lora_B` (8 to 768).\n",
        "  - There’s also an extra `lora_dropout` (10%) alongside the regular attention dropouts, but the core dropout settings stay the same.\n",
        "- **MLP Changes**: In the MLP:\n",
        "  - `c_fc` (768 to 3,072) stays as a regular convolution with no LoRA applied.\n",
        "  - `c_proj` (3,072 to 768) gets the LoRA treatment: wrapped in `lora.Linear` with `lora_A` (3,072 to 8) and `lora_B` (8 to 768), plus `lora_dropout`.\n",
        "  - The GELU activation and regular dropout remain unchanged.\n",
        "- **Score Layer**: The final linear layer (768 to 2) is now wrapped in a `ModulesToSaveWrapper`. This keeps the original layer but adds a mechanism to save specific parts during training, though the layer itself doesn’t change much.\n",
        "- **Key Difference**: With LoRA, most of the original 124 million parameters are frozen. Instead, you only train the small LoRA layers (e.g., `lora_A` and `lora_B`), which might add just thousands of parameters instead of millions. This makes fine-tuning way more efficient.\n",
        "\n",
        "---\n",
        "\n",
        "### The Big Picture\n",
        "- **Before PEFT**: You’ve got a full GPT-2 model where everything can be tweaked. It’s powerful but heavy—every weight gets updated, so you need lots of resources.\n",
        "- **After PEFT**: You’re still using GPT-2, but LoRA sprinkles in these tiny, trainable layers on top of the attention and MLP projections. The bulk of the model stays frozen, and you only adjust these small additions. It’s like giving the model a lightweight upgrade instead of rebuilding the whole thing.\n",
        "\n",
        "In short, the \"After PEFT\" version keeps the same GPT-2 soul but makes it much leaner and faster to fine-tune by focusing on a handful of new parameters rather than the whole beast!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9naPPfF37TN",
        "outputId": "62534322-3f34-44c5-ea8a-a734b1c86864"
      },
      "outputs": [],
      "source": [
        "# the the trainable parameters\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYELjH6h37TN"
      },
      "source": [
        "## Training the PEFT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "gyAF23dR5fn6",
        "outputId": "04649775-4b39-4691-a245-2f5e2720a9f7"
      },
      "outputs": [],
      "source": [
        "# Let's train the model\n",
        "# Train the model\n",
        "# Let's create the trainer\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "HDAG5Wj56b2i",
        "outputId": "3051bb0e-e52a-4850-928b-e1759a6ef0ba"
      },
      "outputs": [],
      "source": [
        "# Let's evaluate the model\n",
        "test_results = trainer.evaluate(tokenized_dataset['test'])\n",
        "print(\"Test Results:\", test_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABg34AC_AjO3",
        "outputId": "b2a0dfab-9f6c-4082-d0ab-0f19cc5bc3fd"
      },
      "outputs": [],
      "source": [
        "# Let's see the post-training score to ensure correct loading of model and it's weight during inference\n",
        "post_training_score = peft_model.score.weight[0][:5].tolist()\n",
        "print(\"Post-training score.weight:\", post_training_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPpHKhG15CqU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Path to the trainer_state.json from the latest checkpoint\n",
        "checkpoint_path = \"./gpt2-lora-rotten-tomatoes-classification/checkpoint-2134\"\n",
        "\n",
        "# Construct the full path to trainer_state.json\n",
        "trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\") # Join the directory with the file name\n",
        "\n",
        "# Load the trainer state\n",
        "with open(trainer_state_path, \"r\") as f:\n",
        "    trainer_state = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WZWqFlIgZMi",
        "outputId": "0ee60e83-92af-419f-ce8f-8183f737efd2"
      },
      "outputs": [],
      "source": [
        "trainer_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q52TwBVN5vPe",
        "outputId": "94dd4891-d092-4b9c-bcdb-13894c2cb3c5"
      },
      "outputs": [],
      "source": [
        "trainer_state['log_history']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "qpMXdjU1gmm1",
        "outputId": "9363713c-d013-4002-d7ba-fc8f0037a486"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "steps = []\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "step_train_loss = None\n",
        "\n",
        "for entry in trainer_state[\"log_history\"]:\n",
        "    if \"loss\" in entry:\n",
        "        step_train_loss = entry[\"loss\"]\n",
        "    if \"eval_loss\" in entry:\n",
        "        steps.append(entry['step'])\n",
        "        val_loss.append(entry[\"eval_loss\"])\n",
        "        train_loss.append(step_train_loss)\n",
        "\n",
        "plt.plot(steps, train_loss, label=\"Train Loss\", color=\"blue\", marker='x')\n",
        "plt.plot(steps, val_loss, label=\"Validation Loss\", color=\"orange\", marker='x')\n",
        "plt.xlabel(\"Eval Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKFEjJaAneV4"
      },
      "source": [
        "### Saving the Model After Training\n",
        "With current `Trainer` setup, we're already saving checkpoints during training (via `save_strategy=\"steps\"` in `training_args`). However, since we’re using PEFT with LoRA, the default saving behavior only saves the LoRA adapter weights—not the full base model—because that’s the efficient part of PEFT. You have two options:\n",
        "\n",
        "1. **Save Only the LoRA Adapters** (smaller footprint, requires base model to load).\n",
        "2. **Save the Full Merged Model** (base model + LoRA weights combined, standalone).\n",
        "\n",
        "I’ll cover both approaches.\n",
        "\n",
        "#### Option 1: Save Only the LoRA Adapters\n",
        "This is the default behavior with PEFT and aligns with your current setup.\n",
        "\n",
        "```python\n",
        "# Your training setup\n",
        "from transformers import Trainer, DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "trainer = Trainer(\n",
        "    model=peft_model,  # Your LoRA-adapted model\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the LoRA adapters and tokenizer to a specific directory\n",
        "output_path = \"/content/drive/MyDrive/NepGPT/PEFT/lora_adapters\"\n",
        "trainer.save_model(output_path)  # Saves LoRA weights and config\n",
        "tokenizer.save_pretrained(output_path)  # Saves tokenizer\n",
        "```\n",
        "\n",
        "- **What’s Saved**:\n",
        "  - `adapter_model.safetensors` (or `.bin`): The LoRA adapter weights.\n",
        "  - `adapter_config.json`: The PEFT configuration (e.g., LoRA rank, target modules).\n",
        "  - Tokenizer files (e.g., `tokenizer.json`, `vocab.txt`).\n",
        "- **Size**: Very small (e.g., a few MB), since only the adapter weights are saved.\n",
        "- **Note**: Your `training_args` already saves checkpoints to `output_dir`, so this is just an explicit final save. The best model (per `load_best_model_at_end=True`) will already be loaded into `peft_model` after `trainer.train()`.\n",
        "\n",
        "#### Option 2: Save the Full Merged Model\n",
        "If you want a standalone model for inference without needing to reload the base model separately, merge the LoRA adapters into the base model and save it.\n",
        "\n",
        "```python\n",
        "# After training\n",
        "trainer.train()\n",
        "\n",
        "# Merge the LoRA weights into the base model\n",
        "merged_model = peft_model.merge_and_unload()  # Merges adapters into base model and unloads PEFT\n",
        "\n",
        "# Save the full merged model and tokenizer\n",
        "output_path = \"/content/drive/MyDrive/NepGPT/PEFT/full_merged_model\"\n",
        "merged_model.save_pretrained(output_path)  # Saves full model weights and config\n",
        "tokenizer.save_pretrained(output_path)  # Saves tokenizer\n",
        "```\n",
        "\n",
        "- **What’s Saved**:\n",
        "  - `model.safetensors` (or `pytorch_model.bin`): The full model weights (base + merged LoRA).\n",
        "  - `config.json`: Model configuration.\n",
        "  - Tokenizer files.\n",
        "- **Size**: Larger (e.g., hundreds of MB for GPT-2), since it includes the entire base model.\n",
        "- **Benefit**: No need to handle PEFT separately during inference.\n",
        "\n",
        "---\n",
        "\n",
        "### Loading the Model for Inference\n",
        "The loading process depends on which saving option you chose.\n",
        "\n",
        "#### Loading Option 1: LoRA Adapters Only\n",
        "You’ll need the base model (e.g., `gpt2`) and the saved LoRA adapters.\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model\n",
        "base_model_name = \"gpt2\"  # Replace with your actual base model name if different\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)\n",
        "\n",
        "# Load the tokenizer\n",
        "adapter_path = \"/content/drive/MyDrive/NepGPT/PEFT/lora_adapters\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "\n",
        "# Load the LoRA adapters and apply them to the base model\n",
        "peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "peft_model.eval()\n",
        "\n",
        "# Example inference\n",
        "inputs = tokenizer(\"This movie is great!\", return_tensors=\"pt\")\n",
        "outputs = peft_model(**inputs)\n",
        "logits = outputs.logits\n",
        "prediction = logits.argmax(-1).item()\n",
        "print(\"Prediction:\", prediction)\n",
        "```\n",
        "\n",
        "- **Key Points**:\n",
        "  - You must specify the original base model (`gpt2` or whatever you used).\n",
        "  - The `PeftModel` combines the base model with the LoRA adapters dynamically.\n",
        "\n",
        "#### Loading Option 2: Full Merged Model\n",
        "If you saved the merged model, loading is simpler—no PEFT required.\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the full merged model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/NepGPT/PEFT/full_merged_model\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Example inference\n",
        "inputs = tokenizer(\"This movie is great!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "prediction = logits.argmax(-1).item()\n",
        "print(\"Prediction:\", prediction)\n",
        "```\n",
        "\n",
        "- **Key Points**:\n",
        "  - No PEFT dependency—just a standard Hugging Face model.\n",
        "  - Ready to use out of the box.\n",
        "\n",
        "---\n",
        "\n",
        "### Recommendation\n",
        "- **Use Option 1 (LoRA Adapters)** if:\n",
        "  - You want to save disk space and share only the adapters.\n",
        "  - You’re fine with loading the base model separately during inference.\n",
        "- **Use Option 2 (Full Merged Model)** if:\n",
        "  - You need a standalone model for deployment or simpler inference.\n",
        "  - Disk space isn’t a concern.\n",
        "\n",
        "Since I am using Google Drive and likely experimenting, I’d start with **Option 1** to keep things lightweight and aligned with PEFT’s philosophy. If you need a one-click inference solution later, switch to Option 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRyOz7vQrtDe",
        "outputId": "09b104f6-f39f-4b2d-aee4-068ca126130b"
      },
      "outputs": [],
      "source": [
        "# Option 1:\n",
        "# Save the LoRA adapters and tokenizer to a specific directory\n",
        "output_path = \"./lora_adapters\"\n",
        "trainer.save_model(output_path)  # Saves LoRA weights and config\n",
        "tokenizer.save_pretrained(output_path)  # Saves tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peVHRDi_r8La",
        "outputId": "18824dd2-89e3-4a22-f8ad-7c6f59850b28"
      },
      "outputs": [],
      "source": [
        "# Option 2:\n",
        "# Merge the LoRA weights into the base model\n",
        "merged_model = peft_model.merge_and_unload()  # Merges adapters into base model and unloads PEFT\n",
        "\n",
        "# Save the full merged model and tokenizer\n",
        "output_path = \"./full_merged_model\"\n",
        "merged_model.save_pretrained(output_path)  # Saves full model weights and config\n",
        "tokenizer.save_pretrained(output_path)  # Saves tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9JUSJP0sNHi"
      },
      "source": [
        "## Inference Using Option 1: LoRA Adapters\n",
        "\n",
        "You might get this warning:\n",
        "`Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.`\n",
        "What is this warning? GPT2 model is used for generating the text not for the classification. But we are using the `gpt2` for classification. So we have score layer in the below architecture which is randomly initialized that acts as the classification head.\n",
        "```python\n",
        "GPT2ForSequenceClassification(\n",
        "  (transformer): GPT2Model(\n",
        "    (wte): Embedding(50257, 768)\n",
        "    (wpe): Embedding(1024, 768)\n",
        "    (drop): Dropout(p=0.1, inplace=False)\n",
        "    (h): ModuleList(\n",
        "      (0-11): 12 x GPT2Block(\n",
        "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        (attn): GPT2Attention(\n",
        "          (c_attn): Conv1D(nf=2304, nx=768)\n",
        "          (c_proj): Conv1D(nf=768, nx=768)\n",
        "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
        "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
        "        )\n",
        "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        (mlp): GPT2MLP(\n",
        "          (c_fc): Conv1D(nf=3072, nx=768)\n",
        "          (c_proj): Conv1D(nf=768, nx=3072)\n",
        "          (act): NewGELUActivation()\n",
        "          (dropout): Dropout(p=0.1, inplace=False)\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "  )\n",
        "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
        ")\n",
        "```\n",
        "When we fin-tune this model using LoRA from the peft library we only choose to fine-tune the layers as shown in the config below:\n",
        "```python\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                    # Rank of the adaptation\n",
        "    lora_alpha=32,          # Scaling factor\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # layers to fine-tune in GPT-2\n",
        "    lora_dropout=0.1,       # Dropout for regularization\n",
        "    fan_in_fan_out=True,    # Match Conv1D expectation, eliminates the warning\n",
        "    task_type=\"SEQ_CLS\"   # For sequence Classification\n",
        ")\n",
        "```\n",
        "Since `score.weight` isn’t present in the gpt2 checkpoint, Hugging Face initializes it randomly when you load `AutoModelForSequenceClassification`. This is what triggers the warning.\n",
        "\n",
        "How to avoid this warning?\n",
        "We can completely ignore this warning as it is seen just while loading the base model before it is fine-tuned and warning the user you might have to TRAIN this model.\n",
        "\n",
        "Also we need to keep in mind that when we initialize the model with `peft_model = PeftModel.from_pretrained(base_model, lora_config)`, the score layer remains a regular Linear layer (not adapted by LoRA).\n",
        "During training, the optimizer (controlled by Trainer) updates `score.weight` fully because unless not frozen or adapted by LoRA. We can check wether the it is optimized or not by\n",
        "```python\n",
        "# After initializing peft_model\n",
        "print(\"Is score.weight trainable?\", peft_model.score.weight.requires_grad)  # Should print True\n",
        "```\n",
        "Therefore, later when you load the adapter after fine-tuning with the base model the completely fine-tuned `score.weight` get's loaded. No need to worry about the warning at all.  \n",
        "\n",
        "Also to be extra cautious we can check after training and after saving the model:\n",
        "```python\n",
        "# After training\n",
        "trainer.train()\n",
        "post_training_score = peft_model.score.weight[0][:5].tolist()\n",
        "print(\"Post-training score.weight:\", post_training_score)\n",
        "peft_model.save_pretrained(output_path)\n",
        "tokenizer.save_pretrained(output_path)\n",
        "\n",
        "# Reload\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\")\n",
        "peft_model = PeftModel.from_pretrained(base_model, output_path)\n",
        "loaded_score = peft_model.score.weight[0][:5].tolist()\n",
        "print(\"Loaded score.weight:\", loaded_score)\n",
        "print(\"Match?\", post_training_score == loaded_score)\n",
        "```\n",
        "If `Match? True`, warning is just noise, and model is working as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GduZFkSlEkUK",
        "outputId": "be3e5002-11cf-4bee-e519-58c2581d31e7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model\n",
        "base_model_name = \"gpt2\"  # Replace with your actual base model name if different\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OZQ3eJWBDpr",
        "outputId": "4be08732-a173-45c3-f090-bec87ad6a8a6"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "adapter_path = \"./lora_adapters\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "\n",
        "with open(f\"{adapter_path}/id2label.json\", \"r\") as f:\n",
        "    id2label = json.load(f)\n",
        "\n",
        "# Load the LoRA adapters and apply them to the base model\n",
        "peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "loaded_score = peft_model.score.weight[0][:5].tolist()\n",
        "print(\"Loaded score.weight:\", loaded_score)\n",
        "print(\"Match?\", post_training_score == loaded_score)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "peft_model.eval()\n",
        "\n",
        "# Example inference\n",
        "inputs = tokenizer(\"This movie is great!\", return_tensors=\"pt\")\n",
        "outputs = peft_model(**inputs)\n",
        "logits = outputs.logits\n",
        "prediction_id = logits.argmax(-1).item()\n",
        "# Convert prediction_id to string before accessing id2label\n",
        "prediction_label = id2label[str(prediction_id)]\n",
        "print(\"Input:\", inputs)\n",
        "print(\"Prediction:\", prediction_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApTZBHHBsau3"
      },
      "source": [
        "## Inference Using Option 2: Full-merged Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Roz37jK7qaVx",
        "outputId": "1c1b3058-5f3e-40ac-929a-17d8b3e7c141"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the full merged model and tokenizer\n",
        "model_path = \"./full_merged_model\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Example inference\n",
        "inputs = tokenizer(\"This movie is great!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "prediction_id = logits.argmax(-1).item()\n",
        "# Convert prediction_id to string before accessing id2label\n",
        "prediction_label = id2label[str(prediction_id)]\n",
        "print(\"Input:\", inputs)\n",
        "print(\"Prediction:\", prediction_label)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nepgpt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d87b2733d7543ed9ed6d09d70a099c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351946fadb9547f89971f354eaca58cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aff379c7bc241aa93df512376d6753c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e062203706b14beaad78a22ca007ff33",
            "placeholder": "​",
            "style": "IPY_MODEL_e3e5196310224bb6a2739c1fb0ca4029",
            "value": " 1066/1066 [00:00&lt;00:00, 3691.42 examples/s]"
          }
        },
        "ad52d1b1b626472aace70f78c586b204": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc5ba5e708c4441ea97ceb78ca22b313": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bea4f2c5222a4dbc90dff37772379e79",
              "IPY_MODEL_f4a9f1d1c03f42b598d09c1a13f15565",
              "IPY_MODEL_3aff379c7bc241aa93df512376d6753c"
            ],
            "layout": "IPY_MODEL_1d87b2733d7543ed9ed6d09d70a099c1"
          }
        },
        "bea4f2c5222a4dbc90dff37772379e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f012ae182b4947c68a52fbbb6ea5473d",
            "placeholder": "​",
            "style": "IPY_MODEL_ad52d1b1b626472aace70f78c586b204",
            "value": "Map: 100%"
          }
        },
        "e062203706b14beaad78a22ca007ff33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3e5196310224bb6a2739c1fb0ca4029": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eea41d1d20a2433dbc9fdc5919fb6329": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f012ae182b4947c68a52fbbb6ea5473d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4a9f1d1c03f42b598d09c1a13f15565": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_351946fadb9547f89971f354eaca58cc",
            "max": 1066,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eea41d1d20a2433dbc9fdc5919fb6329",
            "value": 1066
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
